{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Web Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import urllib\n",
    "from urllib import parse\n",
    "from urllib import request\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import timedelta, date\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the request url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://mrxwlb.com/2016%E5%B9%B41%E6%9C%881%E6%97%A5%E6%96%B0%E9%97%BB%E8%81%94%E6%92%AD%E6%96%87%E5%AD%97%E7%89%88\n"
     ]
    }
   ],
   "source": [
    "def mk_xwlb_url(base='http://mrxwlb.com/{}', y=2016, m=1, d=1):\n",
    "    '''\n",
    "    Make url based on date parameters\n",
    "    The url we're interested in looks like http://mrxwlb.com/2016年1月1日新闻联播文字版\n",
    "    Since it contains Chinese characters, we made a function to format it into proper requests\n",
    "    This function fills year, month and date into the url so that we can properly format 2 years of urls\n",
    "    '''\n",
    "    page_name = '{}年{}月{}日新闻联播文字版'.format(y, m, d)\n",
    "    page_name = parse.quote(page_name)\n",
    "    url = base.format(page_name)\n",
    "    return url\n",
    "\n",
    "\n",
    "# Test if the url is properly made\n",
    "print(mk_xwlb_url(m=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dates of interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.date(2016, 1, 1),\n",
       " datetime.date(2016, 1, 2),\n",
       " datetime.date(2016, 1, 3),\n",
       " datetime.date(2016, 1, 4),\n",
       " datetime.date(2016, 1, 5),\n",
       " datetime.date(2016, 1, 6),\n",
       " datetime.date(2016, 1, 7),\n",
       " datetime.date(2016, 1, 8),\n",
       " datetime.date(2016, 1, 9),\n",
       " datetime.date(2016, 1, 10)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inclusive_daterange(start_date, end_date):\n",
    "    '''\n",
    "    This function takes in a start date and an end date, returns the dates between these two dates\n",
    "    note: start_date and end_date are both datetime.date objects\n",
    "    As its name suggests, the returned dates include start date and end date\n",
    "    '''\n",
    "    results = []\n",
    "    for n in range( int((end_date - start_date).days) + 1 ):\n",
    "        results.append( start_date + timedelta(n) )\n",
    "    return results\n",
    "\n",
    "start_date = date(2016, 1, 1)\n",
    "end_date = date(2017, 12, 31)\n",
    "\n",
    "# test inclusive_daterange function\n",
    "inclusive_daterange(start_date, end_date)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start scraping\n",
    "Note: Show the breaks of the scraping process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 162/731 [05:26<19:06,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 263/731 [08:28<15:04,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-09-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 269/731 [08:38<14:50,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-09-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 731/731 [23:08<00:00,  1.90s/it]\n"
     ]
    }
   ],
   "source": [
    "# we use a dictionary called soups to store all the scraped pages\n",
    "soups = {}\n",
    "\n",
    "for this_date in tqdm(inclusive_daterange(start_date, end_date)):\n",
    "    # Some web pages are missing, we use try and except to deal with broken links that will results in HTTPErrors\n",
    "    try:\n",
    "        # proper url is made to send requests\n",
    "        url = mk_xwlb_url(y=this_date.year, m=this_date.month, d=this_date.day)\n",
    "        \n",
    "        # send request, get back raw html and saved in a variable called html_doc\n",
    "        html_doc = request.urlopen(url)\n",
    "        \n",
    "        # use BeautifulSoup to read / parse html object\n",
    "        soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "        \n",
    "        # save the resulting soup object into the dictionary we made earlier\n",
    "        soups[this_date.strftime('%Y-%m-%d')] = soup\n",
    "    except urllib.error.HTTPError:\n",
    "        # if the requested url results in error, make a note by printing out the date and continue the loop\n",
    "        print(this_date)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the result, the transcripts from 2016-06-10, 2016-09-19, and 2016-09-25 are not found. Given that we have more than 700 transcrpts to go through, skipping three would not undermine the quality of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the texts into a formated data frame\n",
    "1. Data cleaning;\n",
    "2. Assign unique section titles to each paragraph of the respective section;\n",
    "3. Assign dates to each paragraph;\n",
    "4. Note: each section (under a unique title) may have multiple paragraphs; each daily transcript may have multiple sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 728/728 [00:00<00:00, 1010.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>陆军领导机构火箭军战略支援部队成立大会在京举行 习近平向中国人民解放军陆军火箭军战略支援部队...</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>陆军领导机构火箭军战略支援部队成立大会在京举行 习近平向中国人民解放军陆军火箭军战略支援部队...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>中国人民解放军陆军领导机构、中国人民解放军火箭军、中国人民解放军战略支援部队成立大会2015...</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>陆军领导机构火箭军战略支援部队成立大会在京举行 习近平向中国人民解放军陆军火箭军战略支援部队...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>下午4时，成立大会开始，全场高唱国歌。仪仗礼兵护卫着鲜艳军旗，正步行进到主席台前。习近平将军...</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>陆军领导机构火箭军战略支援部队成立大会在京举行 习近平向中国人民解放军陆军火箭军战略支援部队...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>授旗仪式后，习近平致训词。他指出：“成立陆军领导机构、火箭军、战略支援部队，是党中央和中央军...</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>陆军领导机构火箭军战略支援部队成立大会在京举行 习近平向中国人民解放军陆军火箭军战略支援部队...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>习近平强调，陆军是党最早建立和领导的武装力量，历史悠久，敢打善战，战功卓著，为党和人民建立了...</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>陆军领导机构火箭军战略支援部队成立大会在京举行 习近平向中国人民解放军陆军火箭军战略支援部队...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content        date  \\\n",
       "0  陆军领导机构火箭军战略支援部队成立大会在京举行 习近平向中国人民解放军陆军火箭军战略支援部队...  2016-01-01   \n",
       "1  中国人民解放军陆军领导机构、中国人民解放军火箭军、中国人民解放军战略支援部队成立大会2015...  2016-01-01   \n",
       "2  下午4时，成立大会开始，全场高唱国歌。仪仗礼兵护卫着鲜艳军旗，正步行进到主席台前。习近平将军...  2016-01-01   \n",
       "3  授旗仪式后，习近平致训词。他指出：“成立陆军领导机构、火箭军、战略支援部队，是党中央和中央军...  2016-01-01   \n",
       "4  习近平强调，陆军是党最早建立和领导的武装力量，历史悠久，敢打善战，战功卓著，为党和人民建立了...  2016-01-01   \n",
       "\n",
       "                                               title  \n",
       "0  陆军领导机构火箭军战略支援部队成立大会在京举行 习近平向中国人民解放军陆军火箭军战略支援部队...  \n",
       "1  陆军领导机构火箭军战略支援部队成立大会在京举行 习近平向中国人民解放军陆军火箭军战略支援部队...  \n",
       "2  陆军领导机构火箭军战略支援部队成立大会在京举行 习近平向中国人民解放军陆军火箭军战略支援部队...  \n",
       "3  陆军领导机构火箭军战略支援部队成立大会在京举行 习近平向中国人民解放军陆军火箭军战略支援部队...  \n",
       "4  陆军领导机构火箭军战略支援部队成立大会在京举行 习近平向中国人民解放军陆军火箭军战略支援部队...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def soup_to_df(date_of_record, soup):\n",
    "    '''\n",
    "    Convert a soup object into a pandas dataframe for easy storage and export\n",
    "    input: date_of_record: the date of the webpage\n",
    "           soup: the soup object of the webpage of the corresponding date\n",
    "    output: The pandas dataframe of the relevant information.\n",
    "            Each row has 3 columns: title, content and date\n",
    "            date will be the same for the entire dataframe,\n",
    "            title will be the paragraphs (identified by tag p) that are blackened in the webpage\n",
    "            content will be the paragraphs that are not blackened\n",
    "            content will also be associated with the title that appears right before it\n",
    "    '''\n",
    "    # initializations\n",
    "    p_tags = soup.findAll('p')\n",
    "    title = '<UNKNOWN>'\n",
    "    results = []\n",
    "    \n",
    "    # skip the first 2 items as they are placeholders in the webpage\n",
    "    for tag in p_tags[2:]:\n",
    "        tag_str = tag.string\n",
    "        # we found that some of the p-tagged paragraphs are code comments which are not useful. \n",
    "        # This if statement excludes them\n",
    "        if type(tag_str) == bs4.element.Comment:\n",
    "            continue\n",
    "        else:\n",
    "            # Some titles are hidden inside a paragraph\n",
    "            # We find them out and update the title\n",
    "            # Otherwise we treat the paragraph as content\n",
    "            # Note here we define a title's content to be the title itself\n",
    "            for child in tag.children:\n",
    "                if child.string:\n",
    "                    if child.name and child.name == 'strong':\n",
    "                        title = content = child.string.strip()\n",
    "                    else:\n",
    "                        content = child.string.strip()\n",
    "                    # Deal with some irregularities\n",
    "                    if title != '<UNKNOWN>' and content != 'Comments are disabled.':\n",
    "                        results.append({'title': title, 'content': content, 'date': date_of_record})\n",
    "    # return the pandas dataframe\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Concatenate all dates' dataframes into one dataframe and output the data\n",
    "content_df = pd.concat([soup_to_df(k, v) for k, v in tqdm(soups.items())], ignore_index=True)\n",
    "content_df.to_csv(\"../data/1_xwlb_content_title_daily.csv\", index=False, encoding='gb18030')\n",
    "content_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
